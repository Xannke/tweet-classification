{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulvilledieu/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /Users/paulvilledieu/.ekphrasis/stats/twitter/counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /Users/paulvilledieu/.ekphrasis/stats/twitter/counts_2grams.txt\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulvilledieu/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            line = line.strip().split('\\t')\n",
    "            for i in range(1, 4):\n",
    "                line[i] = tokenize(line[i])\n",
    "            if mode == \"train\":\n",
    "                labels.append(emotion2label[line[4]])\n",
    "            conv = line[1:4]\n",
    "            conversations.append(conv)\n",
    "    if mode == \"train\":\n",
    "        return np.array(conversations), np.array(labels)\n",
    "    else:\n",
    "        return np.array(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train = preprocessData('../projet2/train.txt', mode=\"train\")\n",
    "texts_dev, labels_dev = preprocessData('../projet2/dev.txt', mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(file):\n",
    "    embeddingsIndex = {}\n",
    "    dim = 0\n",
    "    with io.open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector \n",
    "            dim = len(embeddingVector)\n",
    "    return embeddingsIndex, dim\n",
    "\n",
    "\n",
    "def getEmbeddingMatrix(wordIndex, embeddings, dim):\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, dim))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingMatrix[i] = embeddings.get(word)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 658129 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "embeddings, dim = getEmbeddings('../projet2/emosense.300d.txt')\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([' '.join(list(embeddings.keys()))])\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts_train, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "labels_categorical_train = to_categorical(np.asarray(y_train))\n",
    "labels_categorical_val = to_categorical(np.asarray(y_val))\n",
    "labels_categorical_dev = to_categorical(np.asarray(labels_dev))\n",
    "\n",
    "\n",
    "def get_sequances(texts, sequence_length):\n",
    "    message_first = pad_sequences(tokenizer.texts_to_sequences(texts[:, 0]), sequence_length)\n",
    "    message_second = pad_sequences(tokenizer.texts_to_sequences(texts[:, 1]), sequence_length)\n",
    "    message_third = pad_sequences(tokenizer.texts_to_sequences(texts[:, 2]), sequence_length)\n",
    "    return message_first, message_second, message_third\n",
    "\n",
    "\n",
    "message_first_message_train, message_second_message_train, message_third_message_train = get_sequances(X_train, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_val, message_second_message_val, message_third_message_val = get_sequances(X_val, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_dev, message_second_message_dev, message_third_message_dev = get_sequances(texts_dev, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/paulvilledieu/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/paulvilledieu/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
    "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def buildModel(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 300)      197439000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 24, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNoise (None, 24, 300)      0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNoise (None, 24, 300)      0           embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128)          186880      gaussian_noise_1[0][0]           \n",
      "                                                                 gaussian_noise_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          186880      gaussian_noise_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           11550       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            124         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 197,824,434\n",
      "Trainable params: 385,434\n",
      "Non-trainable params: 197,439,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from kutilities.callbacks import MetricsCallback, PlottingCallback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "metrics = {\n",
    "    \"f1_e\": (lambda y_test, y_pred:\n",
    "             f1_score(y_test, y_pred, average='micro',\n",
    "                      labels=[emotion2label['happy'],\n",
    "                              emotion2label['sad'],\n",
    "                              emotion2label['angry']\n",
    "                              ])),\n",
    "    \"precision_e\": (lambda y_test, y_pred:\n",
    "                    precision_score(y_test, y_pred, average='micro',\n",
    "                                    labels=[emotion2label['happy'],\n",
    "                                            emotion2label['sad'],\n",
    "                                            emotion2label['angry']\n",
    "                                            ])),\n",
    "}\n",
    "\n",
    "# _datasets = {}\n",
    "# _datasets[\"dev\"] = [[message_first_message_dev, message_second_message_dev, message_third_message_dev],\n",
    "#                     np.array(labels_categorical_dev)]\n",
    "# _datasets[\"val\"] = [[message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "#                     np.array(labels_categorical_val)]\n",
    "\n",
    "# metrics_callback = MetricsCallback(datasets=_datasets, metrics=metrics)\n",
    "\n",
    "# filepath = \"models/bidirectional_LSTM_best_weights_{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True, save_weights_only=False,\n",
    "#                              mode='auto', period=1)\n",
    "# tensorboardCallback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/paulvilledieu/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/20\n",
      "24128/24128 [==============================] - 33s 1ms/step - loss: 0.7755 - acc: 0.6903 - val_loss: 0.4068 - val_acc: 0.8452\n",
      "Epoch 2/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.4290 - acc: 0.8403 - val_loss: 0.3280 - val_acc: 0.8830\n",
      "Epoch 3/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.3607 - acc: 0.8691 - val_loss: 0.3182 - val_acc: 0.8841\n",
      "Epoch 4/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.3293 - acc: 0.8810 - val_loss: 0.2792 - val_acc: 0.8995\n",
      "Epoch 5/20\n",
      "24128/24128 [==============================] - 30s 1ms/step - loss: 0.3023 - acc: 0.8884 - val_loss: 0.2772 - val_acc: 0.9017\n",
      "Epoch 6/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2831 - acc: 0.8968 - val_loss: 0.2666 - val_acc: 0.9038\n",
      "Epoch 7/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2657 - acc: 0.9033 - val_loss: 0.2493 - val_acc: 0.9126\n",
      "Epoch 8/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2529 - acc: 0.9082 - val_loss: 0.2562 - val_acc: 0.9065\n",
      "Epoch 9/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2361 - acc: 0.9135 - val_loss: 0.2469 - val_acc: 0.9111\n",
      "Epoch 10/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2218 - acc: 0.9181 - val_loss: 0.2392 - val_acc: 0.9153\n",
      "Epoch 11/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2124 - acc: 0.9220 - val_loss: 0.2412 - val_acc: 0.9145\n",
      "Epoch 12/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.2009 - acc: 0.9256 - val_loss: 0.2379 - val_acc: 0.9161\n",
      "Epoch 13/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1961 - acc: 0.9283 - val_loss: 0.2437 - val_acc: 0.9164\n",
      "Epoch 14/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1794 - acc: 0.9342 - val_loss: 0.2437 - val_acc: 0.9161\n",
      "Epoch 15/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1700 - acc: 0.9380 - val_loss: 0.2559 - val_acc: 0.9138\n",
      "Epoch 16/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1695 - acc: 0.9377 - val_loss: 0.2367 - val_acc: 0.9176\n",
      "Epoch 17/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1598 - acc: 0.9414 - val_loss: 0.2378 - val_acc: 0.9191\n",
      "Epoch 18/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1514 - acc: 0.9450 - val_loss: 0.2513 - val_acc: 0.9186\n",
      "Epoch 19/20\n",
      "24128/24128 [==============================] - 29s 1ms/step - loss: 0.1408 - acc: 0.9492 - val_loss: 0.2422 - val_acc: 0.9189\n",
      "Epoch 20/20\n",
      "24128/24128 [==============================] - 30s 1ms/step - loss: 0.1340 - acc: 0.9509 - val_loss: 0.2494 - val_acc: 0.9199\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=20,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.7055150884495317\n",
      "precision_e 0.6231617647058824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      2338\n",
      "           1       0.61      0.77      0.69       142\n",
      "           2       0.73      0.82      0.77       125\n",
      "           3       0.57      0.84      0.68       150\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2755\n",
      "   macro avg       0.72      0.84      0.77      2755\n",
      "weighted avg       0.92      0.90      0.91      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
    "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise, RepeatVector\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def buildModel2(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    turn1_branch = RepeatVector(1)(turn1_branch)\n",
    "    turn2_branch = RepeatVector(1)(turn2_branch)\n",
    "    turn3_branch = RepeatVector(1)(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "\n",
    "    x = LSTM(lstm_dim, dropout=dropout_lstm)(x)\n",
    "\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model2 = buildModel2(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_92 (InputLayer)           (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_93 (InputLayer)           (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_94 (InputLayer)           (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_31 (Embedding)        (None, 24, 300)      197439000   input_92[0][0]                   \n",
      "                                                                 input_93[0][0]                   \n",
      "                                                                 input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_91 (GaussianNois (None, 24, 300)      0           embedding_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_92 (GaussianNois (None, 24, 300)      0           embedding_31[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_93 (GaussianNois (None, 24, 300)      0           embedding_31[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_61 (Bidirectional (None, 128)          186880      gaussian_noise_91[0][0]          \n",
      "                                                                 gaussian_noise_93[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_62 (Bidirectional (None, 128)          186880      gaussian_noise_92[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_28 (RepeatVector) (None, 1, 128)       0           bidirectional_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_29 (RepeatVector) (None, 1, 128)       0           bidirectional_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_30 (RepeatVector) (None, 1, 128)       0           bidirectional_61[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 3, 128)       0           repeat_vector_28[0][0]           \n",
      "                                                                 repeat_vector_29[0][0]           \n",
      "                                                                 repeat_vector_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_91 (LSTM)                  (None, 64)           49408       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 64)           0           lstm_91[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 30)           1950        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 4)            124         dense_21[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 197,864,242\n",
      "Trainable params: 425,242\n",
      "Non-trainable params: 197,439,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/15\n",
      " 2600/24128 [==>...........................] - ETA: 1:44 - loss: 1.2897 - acc: 0.4669"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9112b1b47592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     ),\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     batch_size=200)\n\u001b[0m",
      "\u001b[0;32m~/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model2.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=9,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.6843657817109144\n",
      "precision_e 0.58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93      2338\n",
      "           1       0.60      0.77      0.67       142\n",
      "           2       0.61      0.84      0.71       125\n",
      "           3       0.54      0.89      0.68       150\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2755\n",
      "   macro avg       0.68      0.85      0.75      2755\n",
      "weighted avg       0.91      0.89      0.90      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Concatenate, Activation, \\\n",
    "    Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GaussianNoise, RepeatVector, Permute, Reshape, multiply, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, 3))(a)\n",
    "    a = Dense(3, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def buildModel3(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    turn1_branch = RepeatVector(1)(turn1_branch)\n",
    "    turn2_branch = RepeatVector(1)(turn2_branch)\n",
    "    turn3_branch = RepeatVector(1)(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "\n",
    "    x = LSTM(lstm_dim, dropout=dropout_lstm, return_sequences=True)(x)\n",
    "\n",
    "    x = attention_3d_block(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model3 = buildModel3(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_107 (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_108 (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_109 (InputLayer)          (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_36 (Embedding)        (None, 24, 300)      197439000   input_107[0][0]                  \n",
      "                                                                 input_108[0][0]                  \n",
      "                                                                 input_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_106 (GaussianNoi (None, 24, 300)      0           embedding_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_107 (GaussianNoi (None, 24, 300)      0           embedding_36[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_108 (GaussianNoi (None, 24, 300)      0           embedding_36[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_71 (Bidirectional (None, 128)          186880      gaussian_noise_106[0][0]         \n",
      "                                                                 gaussian_noise_108[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_72 (Bidirectional (None, 128)          186880      gaussian_noise_107[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_43 (RepeatVector) (None, 1, 128)       0           bidirectional_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_44 (RepeatVector) (None, 1, 128)       0           bidirectional_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_45 (RepeatVector) (None, 1, 128)       0           bidirectional_71[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 3, 128)       0           repeat_vector_43[0][0]           \n",
      "                                                                 repeat_vector_44[0][0]           \n",
      "                                                                 repeat_vector_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_106 (LSTM)                 (None, 3, 64)        49408       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 64, 3)        0           lstm_106[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 64, 3)        0           permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64, 3)        12          reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 3, 64)        0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 3, 64)        0           lstm_106[0][0]                   \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 192)          0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 192)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 30)           5790        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 4)            124         dense_26[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 197,868,094\n",
      "Trainable params: 429,094\n",
      "Non-trainable params: 197,439,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/paulvilledieu/SCIA/Introduction-DeepLearning/data-science/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/15\n",
      "24128/24128 [==============================] - 49s 2ms/step - loss: 1.0579 - acc: 0.5700 - val_loss: 0.5980 - val_acc: 0.7802\n",
      "Epoch 2/15\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.4830 - acc: 0.8236 - val_loss: 0.3465 - val_acc: 0.8763\n",
      "Epoch 3/15\n",
      "24128/24128 [==============================] - 36s 2ms/step - loss: 0.3698 - acc: 0.8647 - val_loss: 0.2960 - val_acc: 0.8941\n",
      "Epoch 4/15\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.3357 - acc: 0.8769 - val_loss: 0.2951 - val_acc: 0.8972\n",
      "Epoch 5/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.3001 - acc: 0.8918 - val_loss: 0.2628 - val_acc: 0.9062\n",
      "Epoch 6/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2766 - acc: 0.9018 - val_loss: 0.2547 - val_acc: 0.9101\n",
      "Epoch 7/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2690 - acc: 0.9043 - val_loss: 0.2472 - val_acc: 0.9120\n",
      "Epoch 8/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2530 - acc: 0.9088 - val_loss: 0.2473 - val_acc: 0.9115\n",
      "Epoch 9/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2430 - acc: 0.9142 - val_loss: 0.2351 - val_acc: 0.9171\n",
      "Epoch 10/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2301 - acc: 0.9176 - val_loss: 0.2492 - val_acc: 0.9098\n",
      "Epoch 11/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2214 - acc: 0.9223 - val_loss: 0.2365 - val_acc: 0.9193\n",
      "Epoch 12/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.2128 - acc: 0.9250 - val_loss: 0.2483 - val_acc: 0.9118\n",
      "Epoch 13/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.1993 - acc: 0.9293 - val_loss: 0.2332 - val_acc: 0.9198\n",
      "Epoch 14/15\n",
      "24128/24128 [==============================] - 35s 1ms/step - loss: 0.1972 - acc: 0.9300 - val_loss: 0.2493 - val_acc: 0.9159\n",
      "Epoch 15/15\n",
      "24128/24128 [==============================] - 36s 1ms/step - loss: 0.1852 - acc: 0.9363 - val_loss: 0.2350 - val_acc: 0.9174\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=15,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model3.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.6859344894026975\n",
      "precision_e 0.573268921095008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      2338\n",
      "           1       0.54      0.80      0.65       142\n",
      "           2       0.70      0.85      0.77       125\n",
      "           3       0.52      0.91      0.66       150\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2755\n",
      "   macro avg       0.69      0.86      0.75      2755\n",
      "weighted avg       0.92      0.88      0.89      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3284779e-02, 7.2048115e-06, 7.7815901e-04, 9.8592991e-01],\n",
       "       [9.9931324e-01, 6.3388346e-05, 3.7864447e-05, 5.8546365e-04],\n",
       "       [6.7154607e-03, 9.9326718e-01, 1.8156113e-06, 1.5632924e-05],\n",
       "       ...,\n",
       "       [6.0109776e-01, 3.7267306e-05, 1.5390149e-03, 3.9732599e-01],\n",
       "       [9.9379396e-01, 1.5795598e-05, 1.0762960e-04, 6.0825227e-03],\n",
       "       [9.9751461e-01, 4.3820625e-04, 3.1942478e-04, 1.7277488e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_categorical_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Metrics import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [2082.  114.  106.  136.]\n",
      "False Positives per class :  [ 52.  96.  45. 124.]\n",
      "False Negatives per class :  [256.  28.  19.  14.]\n",
      "Class happy : Precision : 0.543, Recall : 0.803, F1 : 0.648\n",
      "Class sad : Precision : 0.702, Recall : 0.848, F1 : 0.768\n",
      "Class angry : Precision : 0.523, Recall : 0.907, F1 : 0.663\n",
      "Ignoring the Others class, Macro Precision : 0.5893, Macro Recall : 0.8525, Macro F1 : 0.6969\n",
      "Ignoring the Others class, Micro TP : 356, FP : 265, FN : 61\n"
     ]
    }
   ],
   "source": [
    "evaluation(y_pred, labels_categorical_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
